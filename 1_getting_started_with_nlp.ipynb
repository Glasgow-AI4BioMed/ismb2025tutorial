{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zStkXUFZwU3L"
      },
      "source": [
        "# Getting started with NLP\n",
        "\n",
        "This hands-on session will provide an introduction to working with text with the spaCy library and applying it to calculate some basic metrics used for document similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4517378",
      "metadata": {},
      "source": [
        "**NOTE:** If you are running this with Colab, you should make a copy for yourself. If you don't, you may lose any edits you make. To make a copy, select `File` (top-left) then `Save a Copy in Drive`. If you are not using Colab, you may need to install some prerequisites. Please see the instructions on the [Github Repo](https://github.com/Glasgow-AI4BioMed/ismb2025tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aea7d50",
      "metadata": {},
      "source": [
        "## Getting Data\n",
        "\n",
        "First we'll download some data that we'll use later on this tutorial with the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cefe1193",
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O data.zip https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EZaU9DTcAwdCpg07eGCIiqMBvGmYJdqfnfhP1ygcDkRkBg?download=1\n",
        "!unzip -qo data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9qJ91NN4Uv-"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "The first thing we'll learn about is splitting text up into tokens. Tokens are similar in concepts to words but also deal with punctuation and other text elements.\n",
        "\n",
        "We'll use the spaCy library which is a commonly used package for processing text. We'll load it up with the standard English model in the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xHP-JaqweVr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e99c95d",
      "metadata": {},
      "source": [
        "Now let's get it to process some text. Applying the `nlp` object to some text parses it and provides a list of all the tokens. We can iterate through them and print them out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8cwe5uGwdKC",
        "outputId": "efae013f-eae8-4700-eef3-2353d5dde689"
      },
      "outputs": [],
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f48b4b",
      "metadata": {},
      "source": [
        "We can get more than just the text of the tokens. The approach above also gives us things like:\n",
        "\n",
        "- The [part of speech](https://en.wikipedia.org/wiki/Part_of_speech) (accessed with `.pos_`) which tells you which tokens are nouns, adjectives, punctuation, etc. spaCy uses the [Universal Dependencies list of parts-of-speech](https://universaldependencies.org/u/pos/).\n",
        "- The [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)) (accessed with `.lemma_`) which gives a canonical version of the word with any suffixes appropriately removed. This means that plurals are turned to singulars, verbs are de-conjugated, etc. This is useful when comparing words so that *run* and *runs* are treated as the same word.\n",
        "\n",
        "Check out the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbubf0L1waXb",
        "outputId": "d7eba7d7-2f45-48cb-e01a-f1a5055da6ee"
      },
      "outputs": [],
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3130ae5",
      "metadata": {},
      "source": [
        "spaCy also enables you to split up text into individual sentences. Often all the information that you want is in a single sentence and it can be easier to process. The sentences can be retrieved with `.sents`. Check out the code below that shows how to print out the individual sentences. You could iterate over each sentence to access the tokens inside each sentence too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFC4cNRNwVAG",
        "outputId": "87bf9245-200e-4862-9821-926a4bc432d2"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. Dr. Smith went to Washington.\n",
        "He arrived at 10 p.m. and started working immediately.\n",
        "\"\"\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for i, sentence in enumerate(doc.sents):\n",
        "    print(f\"{i}: {sentence.text.strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HGwMnDf4vzj"
      },
      "source": [
        "## Task 1\n",
        "\n",
        "Now it is your turn to do some coding with the ideas you've just seen. Here's the task. There are a set of text files in the `data/getting_started` directory. You need to find the longest sentence (by token count) from all the documents. There should be one sentence with noticeably more tokens than the rest. If you get stuck, the answer is provided further down.\n",
        "\n",
        "*Hints: You could use `os.listdir` to get the list of files in that directory and `len(sentence)` gives you the number of tokens in a sentence*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K46S7n_1G5qy"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f19d1a7",
      "metadata": {},
      "source": [
        "\n",
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "Here is the code for the task:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "longest_sentence = []\n",
        "for filename in os.listdir('data/getting_started'):\n",
        "  with open(f'data/getting_started/{filename}') as f:\n",
        "    text = f.read()\n",
        "\n",
        "  doc = nlp(text)\n",
        "\n",
        "  for sentence in doc.sents:\n",
        "    if len(sentence) > len(longest_sentence):\n",
        "      longest_sentence = sentence\n",
        "\n",
        "longest_sentence\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3PRWV9R4zwG"
      },
      "source": [
        "## Measuring similarity with tokens\n",
        "\n",
        "Comparing the tokens between two text sources is a rudimentary but very powerful way to measure their similarity. Let's get the tokens for two sources below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StF93jTQ24NR",
        "outputId": "02b8ec0c-6a73-4bcc-8998-6a055c0b5111"
      },
      "outputs": [],
      "source": [
        "tokens1 = [ token.text for token in nlp(\"She sells seashells on the sea shore.\") ]\n",
        "tokens1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dba1525",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens2 = [ token.text for token in nlp(\"He buys seashells by the sea shore.\") ]\n",
        "tokens2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b090a246",
      "metadata": {},
      "source": [
        "Now we can use Python sets to figure out the tokens that appear in both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e6533d",
      "metadata": {},
      "outputs": [],
      "source": [
        "set(tokens1) & set(tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01471e7",
      "metadata": {},
      "source": [
        "Note that the `&` is shorthand for using the `.intersection` function which in maths uses the $ \\cap $ operator. The above could also be written as `set(tokens1).intersection(tokens2)`.\n",
        "\n",
        "The overlapping tokens are interesting, but what we really want is the count of overlapping tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAYfkrpz3cEG",
        "outputId": "965a4dcf-4e26-42f3-fd60-0c39d7a8fe69"
      },
      "outputs": [],
      "source": [
        "len(set(tokens1) & set(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2babcf4",
      "metadata": {},
      "source": [
        "The higher the number of overlapping tokens, then the higher the similarity (using our assumption that more shared tokens means more similarity). But we'd like to take into account all the tokens, not just the shared ones.\n",
        "\n",
        "Enter the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index). It is a similarity measure for comparing two sets of things, which in our case are tokens. It is defined by the equation $ \\frac{A \\cap B}{A \\cup B} $ where $ A $ and $ B $ are the sets of tokens from the two documents.\n",
        "\n",
        "We've already figured out the top part (numerator) of the Jaccard Index equation which is the number of overlapping tokens. Now we want to calculate the total count of unique tokens that appear in either document. This can be achieved using the `|` operator which is equivalent to `.union` and is the $ \\cup $ operator in maths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycFdkdXx3hji",
        "outputId": "c9e434bc-735f-41db-c127-71866fe309e7"
      },
      "outputs": [],
      "source": [
        "set(tokens1) | set(tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff901e2",
      "metadata": {},
      "source": [
        "Now we can get the counts of the overlapping tokens seen in both sources (using `&`) and the full set of tokens from both sources (using `|`). \n",
        "\n",
        "Let's apply the equation for Jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3REGJbu3eXP",
        "outputId": "47ecbe21-9170-4be9-ad20-822b3319be01"
      },
      "outputs": [],
      "source": [
        "intersection_count = len(set(tokens1) & set(tokens2))\n",
        "union_count = len(set(tokens1) | set(tokens2))\n",
        "jaccard = intersection_count / union_count\n",
        "jaccard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce14b74a",
      "metadata": {},
      "source": [
        "This gives us a score where a higher value means that the two documents are more similar.\n",
        "\n",
        "Let's see what happens with identical documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec61c1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens1 = [ token.text for token in nlp(\"She sells seashells on the sea shore.\") ]\n",
        "tokens2 = [ token.text for token in nlp(\"She sells seashells on the sea shore.\") ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a4796b",
      "metadata": {},
      "source": [
        "When the token sets are the same, the Jaccard index gives a score of one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d168317",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(set(tokens1) & set(tokens2)) / len(set(tokens1) | set(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81734bd",
      "metadata": {},
      "source": [
        "And with no shared tokens, the minimum score will be zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdcb22dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens1 = [ token.text for token in nlp(\"She sells seashells on the sea shore.\") ]\n",
        "tokens2 = [ token.text for token in nlp(\"Very different words\") ]\n",
        "\n",
        "len(set(tokens1) & set(tokens2)) / len(set(tokens1) | set(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dd21d92",
      "metadata": {},
      "source": [
        "The Jaccard Index using tokens is one simple but effective way to measure similarity between documents. There are other options for comparing sets of things (such as the [overlap coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient) and [Dice-Sørensen coefficient](https://en.wikipedia.org/wiki/Dice-S%C3%B8rensen_coefficient)), but we'll stick with the Jaccard for this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTaNFH_sYvpb"
      },
      "source": [
        "## Getting Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d6a82bb",
      "metadata": {},
      "source": [
        "Now let's get our hands on some real biomedical text. The PubMed API can provide a large set of abstracts from published research papers. We'll use the [biopython](https://biopython.org/) library to access it. We need to install that first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6bff61d",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93485f05",
      "metadata": {},
      "source": [
        "The PubMed API requires you to provide your email address. Please fill it in below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jRW0tidY4Ej",
        "outputId": "0185d24a-c7fc-4679-d3dd-ae12e7c2b06a"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "# You need to fill in your email address to use their API\n",
        "Entrez.email = \"\"\n",
        "assert Entrez.email, \"You must put your email address in to Entrez.email as it is a requirement of API usage\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a283de8b",
      "metadata": {},
      "source": [
        "Now let's fetch a single PubMed article. You need to provide the identifiers to request the article, e.g. [a PubMed ID of 31110280](https://pubmed.ncbi.nlm.nih.gov/31110280/). The `Entrez.efetch` function will then request the data from the specific database (in this case pubmed). We also provide some extra details that we want the abstract and the text version of the record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78f5d1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the PubMed ID of the article you want\n",
        "pmid = \"31110280\"  # Example PMID\n",
        "\n",
        "# Fetch the record\n",
        "with Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\", retmode=\"text\") as handle:\n",
        "    abstract = handle.read()\n",
        "\n",
        "print(abstract)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08a5b96",
      "metadata": {},
      "source": [
        "That provides us with the full record. However, in the text mode, it is a little difficult to dig out the individual elements of the record (e.g. the title and abstract). Instead we could use the `xml` format and use `Entrez.read` to parse it for us.\n",
        "\n",
        "With the code below, we can get the title and abstract of the article. There are many fields (e.g. journal title, publication dates, etc) that could also be extracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE4oDDgvZBul",
        "outputId": "4f1df3eb-afc6-41b7-b4f9-9b60423e1e49"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "# Specify PubMed ID\n",
        "pmid = \"31110280\"\n",
        "\n",
        "# Fetch XML record\n",
        "with Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\") as handle:\n",
        "    records = Entrez.read(handle)\n",
        "\n",
        "# Get the article record\n",
        "article = records['PubmedArticle'][0]['MedlineCitation']['Article']\n",
        "\n",
        "# Extract the title\n",
        "title = article['ArticleTitle']\n",
        "\n",
        "# Extract the abstract\n",
        "abstract_text = \" \".join(article['Abstract']['AbstractText'])\n",
        "\n",
        "# Print the results\n",
        "print(f\"Title: {title}\\n\")\n",
        "print(f\"Abstract: {abstract_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ1uDunZxHdS"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "Now, let's combine what you've learned about document similarity with the Jaccard index and using the Pubmed API to get a document. Your task is to find the document in the folder (`data/getting_started`) that has the highest similarity to a Pubmed document (with pmid=38567765). You'll need to use the API to get the document (include the title and abstract as we did before) and get the tokens. Then iterate through the files in the `data/getting_started` directory, tokenize them and calculate the Jaccard index on each one. Then find the file with the highest Jaccard index. \n",
        "\n",
        "As before, the answer is below if you get stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqu8UlhdNBDm"
      },
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c85b270",
      "metadata": {},
      "source": [
        "\n",
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "Here is the code for the task:\n",
        "\n",
        "```python\n",
        "# Let's first get the text for the PubMed document\n",
        "pmid = \"38567765\"\n",
        "\n",
        "with Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\") as handle:\n",
        "    records = Entrez.read(handle)\n",
        "\n",
        "article = records['PubmedArticle'][0]['MedlineCitation']['Article']\n",
        "\n",
        "title = article['ArticleTitle']\n",
        "abstract_text = \" \".join(article['Abstract']['AbstractText'])\n",
        "\n",
        "combined_text = f\"{title}\\n\\n{abstract_text}\\n\"\n",
        "\n",
        "# And tokenize the PubMed document\n",
        "tokens1 = [ token.text for token in nlp(combined_text) ]\n",
        "\n",
        "# Now we'll iterate through the files in data/getting_started\n",
        "best_jaccard, best_file = -1, None\n",
        "for filename in os.listdir('data/getting_started'):\n",
        "  with open(f'data/getting_started/{filename}') as f:\n",
        "    text = f.read()\n",
        "\n",
        "  # Tokenize the text of the document\n",
        "  tokens2 = [ token.text for token in nlp(text) ]\n",
        "\n",
        "  # Calculate the jaccard index\n",
        "  jaccard = len(set(tokens1) & set(tokens2)) / len(set(tokens1) | set(tokens2))\n",
        "  \n",
        "  # And keep track of the document with the highest\n",
        "  if jaccard > best_jaccard:\n",
        "    best_jaccard = jaccard\n",
        "    best_file = filename\n",
        "\n",
        "print(best_jaccard, best_file)\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7165cb1f",
      "metadata": {},
      "source": [
        "## End of Hands-on Session\n",
        "\n",
        "And that brings us to the end of the session. You've learned about:\n",
        "\n",
        "- Tokenization (and getting parts-of-speech, etc)\n",
        "- Sentence splitting\n",
        "- Calculating text similarity using token overlap\n",
        "- Getting a document from PubMed using the API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c662c95",
      "metadata": {},
      "source": [
        "## Optional Extras\n",
        "\n",
        "If you've got extra time, you could try some of the following ideas:\n",
        "\n",
        "- How about other similarity measures such as [Overlap Coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient) or [Dice-Sørensen coefficient](https://en.wikipedia.org/wiki/Dice-S%C3%B8rensen_coefficient)?\n",
        "- Try filtering the tokens to only compare nouns and verbs.\n",
        "- Investigate retrieving multiple documents from the PubMed API\n",
        "- What other metadata can you get from the PubMed API for an article?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
