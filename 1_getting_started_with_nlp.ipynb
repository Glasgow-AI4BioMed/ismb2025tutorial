{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zStkXUFZwU3L"
      },
      "source": [
        "# Getting started with NLP\n",
        "\n",
        "This hands-on session with provide an introduction to working with text with the spaCy library and applying it to calculate some basic metrics used for document similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4517378",
      "metadata": {},
      "source": [
        "**NOTE:** If you are running this with Colab, you should make a copy for yourself. If you don't, you may lose any edits you make. To make a copy, select `File` (top-left) then `Save a Copy in Drive`. If you are not using Colab, you may need to install some prerequisites. Please see the instructions on the [Github Repo](https://github.com/Glasgow-AI4BioMed/ismb2025tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9qJ91NN4Uv-"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "The first thing we'll learn about is splitting text up into tokens. These are similar in concepts to words but also deal with punctuation and other factors.\n",
        "\n",
        "We'll use the spaCy library which is a commonly used package for processing text. We'll load it up with the standard English model in the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xHP-JaqweVr"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e99c95d",
      "metadata": {},
      "source": [
        "Now let's get it to process some text. Applying the `nlp` object to some text parses it and provides a list of all the tokens. We can iterate through them and print them out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8cwe5uGwdKC",
        "outputId": "efae013f-eae8-4700-eef3-2353d5dde689"
      },
      "outputs": [],
      "source": [
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f48b4b",
      "metadata": {},
      "source": [
        "We get more than just the tokens. The default approach above also gives use things like:\n",
        "\n",
        "- The [part of speech](https://en.wikipedia.org/wiki/Part_of_speech) (accessed with `.pos_`) which tells you which tokens are nouns, adjectives, punctuation, etc. spaCy uses the [https://universaldependencies.org/u/pos/](Universal Dependencies list of parts-of-speech).\n",
        "- The [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)) (accessed with `.lemma_`) which gives a canonical version of the word with any suffixes appropriately removed. This means that plurals are turned to singulars, verbs are de-conjugated, etc. This is useful when comparing words so that *run* and *runs* are treated as the same word.\n",
        "\n",
        "Check out the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbubf0L1waXb",
        "outputId": "d7eba7d7-2f45-48cb-e01a-f1a5055da6ee"
      },
      "outputs": [],
      "source": [
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3130ae5",
      "metadata": {},
      "source": [
        "spaCy also enables you to split up text into individual sentences. Often all the information that you want is in a single sentence and it can be easier to process. The sentences can be retrieved with `.sents`. Check out the code below that shows how to print out the individual sentences. You could iterate on each sentence to access the tokens inside each sentence too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFC4cNRNwVAG",
        "outputId": "87bf9245-200e-4862-9821-926a4bc432d2"
      },
      "outputs": [],
      "source": [
        "# Example paragraph\n",
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog. Dr. Smith went to Washington.\n",
        "He arrived at 10 p.m. and started working immediately.\n",
        "\"\"\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print sentences\n",
        "for i, sentence in enumerate(doc.sents):\n",
        "    print(f\"{i}: {sentence.text.strip()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HGwMnDf4vzj"
      },
      "source": [
        "## Task\n",
        "\n",
        "Now it is your turn to do some coding with the ideas you've just seen. Here's the task. You've got a set of text files. You need to find the longest sentence (by token count) from all the documents. There should be one sentence with noticeably more tokens than the rest. If you get stuck, the answer is provided further down."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K46S7n_1G5qy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "longest_sentence = []\n",
        "for filename in os.listdir('task1'):\n",
        "  with open(f'task1/{filename}') as f:\n",
        "    text = f.read()\n",
        "\n",
        "  doc = nlp(text)\n",
        "\n",
        "  for sentence in doc.sents:\n",
        "    if len(sentence) > len(longest_sentence):\n",
        "      longest_sentence = sentence\n",
        "\n",
        "longest_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f19d1a7",
      "metadata": {},
      "source": [
        "\n",
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "Here is the hidden answer or hint!\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3PRWV9R4zwG"
      },
      "source": [
        "## Measuring similarity with tokens\n",
        "\n",
        "Comparing the tokens between two text sources is a rudimentary but very powerful way to measure their similarity. Let's get the tokens for two sources below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StF93jTQ24NR",
        "outputId": "02b8ec0c-6a73-4bcc-8998-6a055c0b5111"
      },
      "outputs": [],
      "source": [
        "tokens1 = [ token.text for token in nlp(\"She sells seashells on the sea shore.\") ]\n",
        "tokens1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dba1525",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens2 = [ token.text for token in nlp(\"He buys seashells by the sea shore.\") ]\n",
        "tokens2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b090a246",
      "metadata": {},
      "source": [
        "Now we can use Python sets to figure out the tokens that appear in both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e6533d",
      "metadata": {},
      "outputs": [],
      "source": [
        "set(tokens1) & set(tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01471e7",
      "metadata": {},
      "source": [
        "Note that the `&` is shorthand for using the `.intersection` function which in maths uses the $ \\cap $ operator. The above could also be written as `set(tokens1).intersection(tokens2)`.\n",
        "\n",
        "The overlapping tokens are interesting, but what we really want is the count of overlapping tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAYfkrpz3cEG",
        "outputId": "965a4dcf-4e26-42f3-fd60-0c39d7a8fe69"
      },
      "outputs": [],
      "source": [
        "len(set(tokens1) & set(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2babcf4",
      "metadata": {},
      "source": [
        "And we want to compare that with all the tokens seen across both text sources. This can be achieved with the `|` operator which is equivalent to `.union` and is the $ \\cup $ operator in maths:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycFdkdXx3hji",
        "outputId": "c9e434bc-735f-41db-c127-71866fe309e7"
      },
      "outputs": [],
      "source": [
        "set(tokens1) | set(tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff901e2",
      "metadata": {},
      "source": [
        "Now we can get the counts of the overlapping tokens seen in both sources (using `&`) and the full set of tokens from both sources (using `|`). We can use the counts of those to calculate the Jaccard index: $ \\frac{A \\cap B}{A \\cup B} $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3REGJbu3eXP",
        "outputId": "47ecbe21-9170-4be9-ad20-822b3319be01"
      },
      "outputs": [],
      "source": [
        "len(set(tokens1) & set(tokens2)) / len(set(tokens1) | set(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce14b74a",
      "metadata": {},
      "source": [
        "This gives us a score where a higher value means that the two documents are more similar. Let's see what happens with equal documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec61c1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens1 = [ token.text for token in nlp(\"She sells seashells on the sea shore.\") ]\n",
        "tokens2 = tokens1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16a4796b",
      "metadata": {},
      "source": [
        "When the token sets are the same, the Jaccard index gives a score of one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d168317",
      "metadata": {},
      "outputs": [],
      "source": [
        "len(set(tokens1) & set(tokens2)) / len(set(tokens1) | set(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81734bd",
      "metadata": {},
      "source": [
        "And with no shared tokens, the minimum score will be zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdcb22dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens1 = [ token.text for token in nlp(\"She sells seashells on the sea shore.\") ]\n",
        "tokens2 = [ token.text for token in nlp(\"Very different words\") ]\n",
        "\n",
        "len(set(tokens1) & set(tokens2)) / len(set(tokens1) | set(tokens2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTaNFH_sYvpb"
      },
      "source": [
        "## Getting Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d6a82bb",
      "metadata": {},
      "source": [
        "Now let's get our hands on some real biomedical text. The PubMed API can provide a large set of abstracts from published research papers. We'll use the [biopython](https://biopython.org/) library to access it. The PubMed API requires you to provide your email address. Please fill it in below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jRW0tidY4Ej",
        "outputId": "0185d24a-c7fc-4679-d3dd-ae12e7c2b06a"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "# Always set your email address\n",
        "Entrez.email = \"jake.lever@glasgow.ac.uk\"\n",
        "assert Entrez.email, \"You must put your email address in to Entrez.email as it is a requirement of API usage\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a283de8b",
      "metadata": {},
      "source": [
        "Now let's fetch a single PubMed article. You need to provide the identifiers to request the article, e.g. [a PubMed ID of 31110280](https://pubmed.ncbi.nlm.nih.gov/31110280/). The `Entrez.efetch` function will then request the data from the specific database (in this case pubmed). We also provide some extra details that we want the abstract and the text version of the record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78f5d1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Specify the PubMed ID of the article you want\n",
        "pmid = \"31110280\"  # Example PMID\n",
        "\n",
        "# Fetch the record\n",
        "with Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"abstract\", retmode=\"text\") as handle:\n",
        "    abstract = handle.read()\n",
        "\n",
        "print(abstract)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08a5b96",
      "metadata": {},
      "source": [
        "That provides us with the full record. However, in the text mode, it is a little difficult to dig out the individual elements of the record (e.g. the title and abstract). Instead we could use the `xml` format and use `Entrez.read` to parse it for us.\n",
        "\n",
        "With the code below, we can get the title and abstract of the article. There are many of fields (e.g. journal title, publication dates, etc) that could also be extracted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE4oDDgvZBul",
        "outputId": "4f1df3eb-afc6-41b7-b4f9-9b60423e1e49"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "# Specify PubMed ID\n",
        "pmid = \"31110280\"\n",
        "\n",
        "# Fetch XML record\n",
        "with Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\") as handle:\n",
        "    records = Entrez.read(handle)\n",
        "\n",
        "# Get the article record\n",
        "article = records['PubmedArticle'][0]['MedlineCitation']['Article']\n",
        "\n",
        "# Extract the title\n",
        "title = article['ArticleTitle']\n",
        "\n",
        "# Extract the abstract\n",
        "abstract_text = \" \".join(article['Abstract']['AbstractText'])\n",
        "\n",
        "# Print the results\n",
        "print(f\"Title: {title}\\n\")\n",
        "print(f\"Abstract: {abstract_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ1uDunZxHdS"
      },
      "source": [
        "## Task\n",
        "\n",
        "Now, let's combine what you've learned about document similarity with the Jaccard index and using the Pubmed API to get a document. Your task is to find the document in the folder that has the highest similarity to a Pubmed document (with pmid=38567765). The similarity should use Jaccard index and be based on text that contain the title and abstract together. As before, the answer is below if you get stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqu8UlhdNBDm"
      },
      "outputs": [],
      "source": [
        "from Bio import Entrez\n",
        "\n",
        "# Specify PubMed ID\n",
        "pmid = \"38567765\"\n",
        "\n",
        "# Fetch XML record\n",
        "with Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"xml\") as handle:\n",
        "    records = Entrez.read(handle)\n",
        "\n",
        "# Get the article record\n",
        "article = records['PubmedArticle'][0]['MedlineCitation']['Article']\n",
        "\n",
        "# Extract the title\n",
        "title = article['ArticleTitle']\n",
        "\n",
        "# Extract the abstract\n",
        "abstract_text = \" \".join(article['Abstract']['AbstractText'])\n",
        "\n",
        "# Print the results\n",
        "combined_text = f\"{title}\\n\\n{abstract_text}\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1jcWC9_V13O"
      },
      "outputs": [],
      "source": [
        "tokens1 = [ token.text for token in nlp(combined_text) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cKfJnZ0V5Pz",
        "outputId": "5d6dd0e4-4053-4e24-f17e-77a01cada998"
      },
      "outputs": [],
      "source": [
        "best_jaccard, best_file = -1, None\n",
        "for filename in os.listdir('task1'):\n",
        "  with open(f'task1/{filename}') as f:\n",
        "    text = f.read()\n",
        "\n",
        "  tokens2 = [ token.text for token in nlp(text) ]\n",
        "\n",
        "  jaccard = len(set(tokens1) & set(tokens2)) / len(set(tokens1) | set(tokens2))\n",
        "\n",
        "  if jaccard > best_jaccard:\n",
        "    best_jaccard = jaccard\n",
        "    best_file = filename\n",
        "\n",
        "best_jaccard, best_file"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7165cb1f",
      "metadata": {},
      "source": [
        "## End of Hands-on Session\n",
        "\n",
        "And that brings us to the end of the session. You've learned about:\n",
        "\n",
        "- Tokenization (and getting parts-of-speech, etc)\n",
        "- Sentence splitting\n",
        "- Calculating text similarity using token overlap\n",
        "- Getting a document from PubMed using the API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c662c95",
      "metadata": {},
      "source": [
        "### Optional Extras\n",
        "\n",
        "If you've got extra time, you could some different approaches.\n",
        "\n",
        "- How about other similarity measures such as [Overlap Coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient) or [Dice-Sørensen coefficient](https://en.wikipedia.org/wiki/Dice-S%C3%B8rensen_coefficient)?\n",
        "- Try filtering the tokens to only compare nouns and verbs.\n",
        "- Investigate retrieving multiple documents from the PubMed API\n",
        "- What other metadata can you get from the PubMed API for an article?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
