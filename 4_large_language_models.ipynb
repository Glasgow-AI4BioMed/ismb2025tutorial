{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9AKGWc6dU6l"
      },
      "source": [
        "# Large language models for information extraction\n",
        "\n",
        "This hands-on session introduces the different ways to run a large language model (LLM) and explores how they can be applied to biomedical information extraction. There are two main ways to use an LLM: locally and through an API. They both have their pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a0cdc5",
      "metadata": {},
      "source": [
        "**NOTE:** If you are running this with Colab, you should make a copy for yourself. If you don't, you may lose any edits you make. To make a copy, select `File` (top-left) then `Save a Copy in Drive`. If you are not using Colab, you may need to install some prerequisites. Please see the instructions on the [Github Repo](https://github.com/Glasgow-AI4BioMed/ismb2025tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946aa4f6",
      "metadata": {},
      "source": [
        "## Getting Data\n",
        "\n",
        "As in the previous sessions, we'll download some data that we'll use later on this tutorial with the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bb66e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O data.zip https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EZaU9DTcAwdCpg07eGCIiqMBvGmYJdqfnfhP1ygcDkRkBg?download=1\n",
        "!unzip -qo data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ae25c7",
      "metadata": {},
      "source": [
        "## Getting a GPU\n",
        "\n",
        "This session will make use of the free GPU available on Google Colab. To get it..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e913c8e5",
      "metadata": {},
      "source": [
        "## Loading data\n",
        "\n",
        "We've prepared some text that contains various information that we can be extracted. Let's load it up and take a quick look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd580f5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "with open('data/llm_sentences.json') as f:\n",
        "  sentences = json.load(f)\n",
        "\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601182de",
      "metadata": {},
      "source": [
        "And if we look at a few of the sentences, we can see that they contain various biomedical entities (e.g. drugs, genes, etc) and relations between them that we might want to extract:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef5f0100",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SydBmlcS-IAb"
      },
      "source": [
        "## Running an LLM locally\n",
        "\n",
        "The first way we'll examine to run an LLM is locally, on the machine that you're using. This has some advantages and disadvantages. Firstly, if you are running the code on a machine you control, then you are able to use sensitive data (e.g. patient medical records) which typically can't be transmitted outside an organisation. It also gives you fine-grained control over the LLM and can enable some adjustments to the LLM. However, it potentially requires a large GPU (or GPUs) and the best performing models are not publicly available.\n",
        "\n",
        "LLMs are typically measured in size by the number of parameters in them. LLMs are largely composed of a huge number of matrix multiplications, and the count of values across all their internal matrices gives the parameter count. The largest best performing models are measured in the hundreds of billions - which requires multiple of the most expensive GPUs to run. Smaller models in the 1B to 70B range can be run on more modest GPUs (with a few tricks).\n",
        "\n",
        "We'll use a smaller 1B model today. The code below loads up the Falcon3-1B-Instruct model. It may take a minute to download the model and its associated tokenizer. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "038d4ba1",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da560b35",
      "metadata": {},
      "source": [
        "Notably in the code above, we tell the code to load the model with float16. Typically computers store values with four bytes (known as float32) so we're asking it to use a smaller representation for the model. There's lots of research that shows that LLMs can work very well even when their internal parameters are compressed down to only a few bits.\n",
        "\n",
        "Let's check how many parameters there are in the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49c15360",
      "metadata": {},
      "source": [
        "Roughly 1.7 billion parameters. How much memory does that need?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d32d9205",
      "metadata": {},
      "outputs": [],
      "source": [
        "bytes = model.num_parameters() * 2\n",
        "gigabytes = bytes / (1024*1024*1024)\n",
        "gigabytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4998a77",
      "metadata": {},
      "source": [
        "So a small model still needs over 3 gigabytes of memory. That's quite a bit of GPU memory.\n",
        "\n",
        "Now we want to use it. Let's load it into a text-generation pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9baa0e",
      "metadata": {},
      "source": [
        "This code actually auto-detects if you have a GPU and uses it when appropriate. However, a lot of code requries you to explicitly tell it to use a GPU. It's always a good idea to check if you're actually using the GPU or your code could be very slow.\n",
        "\n",
        "Now let's put together a query. We'll focus on information extraction use-cases where we have some text that we want to extract information from. Let's look at a simple binary classification of whether a sentence contains a drug:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_text = 'EGFR binds to the EGFR receptor.'\n",
        "\n",
        "query = f\"{sentence_text}\\n\\nDoes the prior text contain a drug? Answer only Yes Or No\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbc5132d",
      "metadata": {},
      "source": [
        "Many LLMs are instruction-tuned which means that they have been specialised to take instructions in the form of a chat. So we need to make sure our instruction gets put in the right form. In this case, we don't pass the query in directly, but pass in the form of a list of messages. The first message is known as a system prompt that gives instructions to the LLM about its general task.\n",
        "\n",
        "Let's create a system prompt and then put in our query. Notice how the role is `system` for the system prompt and then `user` for our message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a friendly chatbot\"},\n",
        "    {\"role\": \"user\", \"content\": query},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171793d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
        "print(tokenizer.decode(tokenized_chat[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = generator(messages, max_new_tokens=1)\n",
        "generated_message = result[0]['generated_text'][-1]\n",
        "\n",
        "print(generated_message['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYF6Clho-uUz"
      },
      "source": [
        "## Using an LLM through an API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSsVOH3v6ig6"
      },
      "source": [
        "https://aistudio.google.com/apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "GEMINI_API_KEY = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.generativeai import GenerativeModel\n",
        "\n",
        "model = GenerativeModel(\"gemma-3-27b-it\")\n",
        "\n",
        "prompt = \"What Disease Ontology (DOID) term does GBM match to?\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MXWKcYWhRP"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUBv-3aWZLFD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c465d094",
      "metadata": {},
      "source": [
        "## Optional Extras\n",
        "\n",
        "If you've got extra time, you could try some of the following ideas:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d468bc29",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
