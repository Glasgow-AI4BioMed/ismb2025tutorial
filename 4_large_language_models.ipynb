{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9AKGWc6dU6l"
      },
      "source": [
        "# Large language models for information extraction (Part 1)\n",
        "\n",
        "This hands-on session introduces the different ways to run a large language model (LLM) and explores how they can be applied to biomedical information extraction. There are two main ways to use an LLM: locally and through an API. They both have their pros and cons."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59a0cdc5",
      "metadata": {},
      "source": [
        "**NOTE:** If you are running this with Colab, you should make a copy for yourself. If you don't, you may lose any edits you make. To make a copy, select `File` (top-left) then `Save a Copy in Drive`. If you are not using Colab, you may need to install some prerequisites. Please see the instructions on the [Github Repo](https://github.com/Glasgow-AI4BioMed/ismb2025tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ae25c7",
      "metadata": {},
      "source": [
        "## Getting a GPU\n",
        "\n",
        "This session will make use of the free GPU available on Google Colab. To get it, select `Edit` in the top-left, then `Notebook Settings`, select `T4 GPU` under Hardware accelerator and click `Save`.\n",
        "\n",
        "This gets you a small GPU that is sufficient for this session. Note that you may get timed out if you leave the notebook inactive for too long."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946aa4f6",
      "metadata": {},
      "source": [
        "## Getting Data\n",
        "\n",
        "As in the previous sessions, we'll download some data that we'll use later on this tutorial with the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bb66e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O data.zip https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/Ec8sygSj-zlAj6RXZHYGXqYBygZYM978Ts2FrTgBTfqmOQ?download=1\n",
        "!unzip -qo data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SydBmlcS-IAb"
      },
      "source": [
        "## Running an LLM locally\n",
        "\n",
        "The first way we'll examine to run an LLM is locally, on the machine that you're using. This has some advantages and disadvantages. Firstly, if you are running the code on a machine you control, then you are able to use sensitive data (e.g. patient medical records) which typically can't be transmitted outside an organisation. It also gives you fine-grained control over the LLM and can enable some adjustments to the LLM. However, it potentially requires a large GPU (or GPUs) and the best performing models are not publicly available.\n",
        "\n",
        "LLMs are typically measured in size by the number of parameters in them. LLMs are largely composed of a huge number of matrix multiplications, and the count of values across all their internal matrices gives the parameter count. The largest best performing models are measured in the hundreds of billions - which requires multiple of the most expensive GPUs to run. Smaller models in the 1B to 70B range can be run on more reasonable GPUs (with a few tricks).\n",
        "\n",
        "Hugging Face's [transformers library](https://github.com/huggingface/transformers) is the main way to get access to transformer-based models. It can also be a bit verbose so for this hands-on session, we'll turn off some of its output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "741e6925",
      "metadata": {},
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a791d1b4",
      "metadata": {},
      "source": [
        "We'll use a smaller 1B model today. The code below loads up the [Falcon3-1B-Instruct model](https://huggingface.co/tiiuae/Falcon3-1B-Instruct). It may take a minute to download the model and its associated tokenizer.\n",
        "\n",
        "*Tip: You may get warnings about HF_TOKEN not existing. These can be ignored.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"tiiuae/Falcon3-1B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da560b35",
      "metadata": {},
      "source": [
        "Notably in the code above, we tell the code to load the model with float16. Typically computers store numbers with four bytes (known as float32) so we're asking it to use a smaller representation for the model (float16 = 2 bytes per number). There's lots of research that shows that LLMs can work very well even when their internal parameters are compressed down to only a few bits.\n",
        "\n",
        "Let's check how many parameters there are in the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49c15360",
      "metadata": {},
      "source": [
        "Roughly 1.7 billion parameters. How much memory does that need (using 2 bytes per parameter)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d32d9205",
      "metadata": {},
      "outputs": [],
      "source": [
        "bytes = model.num_parameters() * 2\n",
        "gigabytes = bytes / (1024*1024*1024)\n",
        "gigabytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4998a77",
      "metadata": {},
      "source": [
        "So a small model still needs over 3 gigabytes of memory. That's quite a bit of GPU memory.\n",
        "\n",
        "Now we want to use it. Let's load it into a text-generation pipeline. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9baa0e",
      "metadata": {},
      "source": [
        "This code above tells the code to explicitly use the GPU on this machine (with `device='cuda'`). Most code requires you to explicitly tell it to use the GPU. It's a good idea to check if you're actually using the GPU or your code could be very slow.\n",
        "\n",
        "Now let's put together a query. We'll focus on information extraction use-cases where we have some text that we want to extract information from. Let's look at a simple binary classification of whether a sentence contains a drug:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_text = \"Lung cancer is broadly classified into two main types: non-small cell lung cancer (NSCLC) and small cell lung cancer (SCLC).\"\n",
        "\n",
        "query = f\"{sentence_text}\\n\\nDoes the prior text contain a disease? Answer only Yes Or No\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbc5132d",
      "metadata": {},
      "source": [
        "Many LLMs are instruction-tuned which means that they have been specialised to take instructions in the form of a chat. So we need to make sure our instruction gets put in the right form. In this case, we don't pass the query in directly, but pass in the form of a list of messages. The first message is known as a system prompt that gives instructions to the LLM about its general task.\n",
        "\n",
        "Let's create a system prompt and then put in our query. Notice how the role is `system` for the system prompt and then `user` for our message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful biomedical lab scientist\"},\n",
        "    {\"role\": \"user\", \"content\": query},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a13da3",
      "metadata": {},
      "source": [
        "Language models work with strings, not the data structure above. So each LLM actually has its own way of formatting a series of messages into a single string. Let's see how this LLM's tokenizer turns those messages into a string to be passed to the LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "171793d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)\n",
        "print(tokenizer.decode(tokenized_chat))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee73f6b",
      "metadata": {},
      "source": [
        "That formatting can actually be done behind the scenes. To use our LLM, we can pass in the messages directly. We'll set a few parameters:\n",
        "\n",
        "- `max_new_tokens=1` : The query asked for a response of only Yes or No, so let's ask for only one token.\n",
        "- `do_sample=False` : Language models produce probability distributions to pick the next word. We'll tell it to always pick the highest probability, and not sample from that distribution. That makes it deterministic.\n",
        "- `return_full_text=False` : We only care about the generated text, so we'll tell it to not return the original text as well.\n",
        "\n",
        "Let's run it on our query (and ignore any warnings you get)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Asking if there is a disease mentioned in \"Lung cancer is broadly classified into two main types: non-small cell lung cancer (NSCLC) and small cell lung cancer (SCLC).\"\n",
        "\n",
        "result = generator(messages, max_new_tokens=1, do_sample=False, return_full_text=False)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99e2f659",
      "metadata": {},
      "source": [
        "Yay, it seems to have identified that there is a disease mention in that sentence (i.e. lung cancer).\n",
        "\n",
        "Let's put those steps together into a function to make it easier to work with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_query(query, max_new_tokens=1):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a biomedical scientist\"},\n",
        "      {\"role\": \"user\", \"content\": query},\n",
        "  ]\n",
        "\n",
        "  result = generator(messages, max_new_tokens=max_new_tokens, do_sample=False, return_full_text=False)\n",
        "\n",
        "  generated_text = result[0]['generated_text']\n",
        "\n",
        "  return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55d9dc1",
      "metadata": {},
      "source": [
        "We'll try running the same inquiry with a different sentence. This sentence doesn't contain a mention of a disease. Will our LLM be able to identify that?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c2eed1",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_text = \"Penicillin's development dates back to Alexander Fleming's work in 1928.\"\n",
        "\n",
        "query = f\"{sentence_text}\\n\\nDoes the prior text contain a disease? Answer only Yes Or No\"\n",
        "\n",
        "run_query(query, max_new_tokens=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05a21a4",
      "metadata": {},
      "source": [
        "Excellent. It correctly identified that no disease is mentioned there.\n",
        "\n",
        "Let's push it. We'll ask it to extract diseases from a longer passage. Let's see what it does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d8150a",
      "metadata": {},
      "outputs": [],
      "source": [
        "long_text = \"\"\"\n",
        "Dysregulation of TP53, BRCA1, and PTEN in conjunction with aberrant expression of EGFR, KRAS, and\n",
        "PIK3CA has been implicated in the pathogenesis of triple-negative breast cancer (TNBC) and non-small\n",
        "cell lung carcinoma (NSCLC), particularly when exacerbated by chronic exposure to benzo[a]pyrene,\n",
        "formaldehyde, and arsenic trioxide, leading to increased genomic instability, enhanced epithelial-\n",
        "to-mesenchymal transition (EMT), and resistance to cisplatin, paclitaxel, and immune checkpoint\n",
        "inhibitors targeting PD-1 and CTLA-4, thereby necessitating combinatorial therapeutic strategies\n",
        "incorporating PARP inhibitors, MEK inhibitors, and monoclonal antibodies against VEGF, especially in\n",
        "patients with co-morbidities such as type 2 diabetes mellitus, hepatitis C, and systemic lupus\n",
        "erythematosus, which further modulate the tumor microenvironment through altered cytokine profiles\n",
        "including IL-6, TNF-Œ±, and TGF-Œ≤1.\n",
        "\"\"\"\n",
        "\n",
        "query = f\"{long_text}\\n\\nExtract the diseases from the previous text.\"\n",
        "\n",
        "run_query(query, max_new_tokens=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1169e0d",
      "metadata": {},
      "source": [
        "Very cool. It seems to have identified some of the diseases in the sentence, though it may missed some (e.g. type 2 diabetes, etc). This is a smaller LLM (1B parameters) which will limit it. We'll try a larger LLM later in this hands-on session."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b444e2",
      "metadata": {},
      "source": [
        "### üìã Task 1: Disease sentences\n",
        "\n",
        "We'll return to the task of identifying if a sentence contains a disease. The LLM worked well for the two sentences that we used and was correct for both. However, we should benchmark its performance with some more sentences and see how it does, and if we can get it any better.\n",
        "\n",
        "Let's load up a dataset of sentences. These are derived from the [BC5CDR dataset](https://pmc.ncbi.nlm.nih.gov/articles/PMC4860626/) which is available [here](https://ftp.ncbi.nlm.nih.gov/pub/lu/BC5CDR/).\n",
        "\n",
        "Our subset contains 200 sentences and whether they contain a mention of a disease. See the structure below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb3664f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('data/llm_disease_sentences.json') as f:\n",
        "  sentences = json.load(f)\n",
        "\n",
        "sentences[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ec673f3",
      "metadata": {},
      "source": [
        "Your task is to evaluate our LLM for the task of identifying sentences mentioning disease. Take the query below and apply it to all 200 sentences. Then count how many times it's answer (e.g. saying Yes) matches with the correct label in the dataset (`sentence['has_disease']`). Calculate what percentage of the sentences it gets it correct for. You should find that it is 60%.\n",
        "\n",
        "```python\n",
        "query = f\"{sentence_text}\\n\\nDoes the prior text contain a disease? Answer only Yes Or No\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9561aff1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec42c506",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üîë Click to see the answer üîë</summary>\n",
        "\n",
        "Here is the code for the task:\n",
        "\n",
        "```python\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "correct, total = 0, 0\n",
        "for sentence in tqdm(sentences):\n",
        "\n",
        "  query = f\"{sentence['text']}\\n\\nDoes the prior text mention a disease? Answer only Yes Or No\"\n",
        "\n",
        "  result = run_query(query, max_new_tokens=1)\n",
        "  prediction = (result == 'Yes')\n",
        "\n",
        "  if prediction == sentence['has_disease']:\n",
        "    correct += 1\n",
        "  total += 1\n",
        "\n",
        "print(f\"{correct}/{total} = {correct/total:.1%}\")\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260a3061",
      "metadata": {},
      "source": [
        "### Optional Extra\n",
        "\n",
        "- You could try adjusting the query. This is known as prompt engineering. Can you improve the performance at all? If you can the performance over 70% you're doing well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYF6Clho-uUz"
      },
      "source": [
        "## Using an LLM through an API\n",
        "\n",
        "The other way to use a large language model is through a company's API. This enables you to use much larger LLMs which require huge GPU resources. However, it may also cost money and can have some limits on how many queries you can make.\n",
        "\n",
        "We'll use one of the free LLMs available through Google's AI Studio. There is no strong reason to use a Google LLM versus an OpenAI or Anthropic (or other provider). Generally you want to benchmark different ones for your task and decide with that information. We're using Google models today as they offer some free models and getting the API key will (hopefully) be easy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f109bc24",
      "metadata": {},
      "source": [
        "### Getting an API key\n",
        "\n",
        "All services require you to get an API key, so let's get an API key from the AI Studio. Follow the steps below:\n",
        "\n",
        "- Click this link: https://aistudio.google.com/apikey\n",
        "- Click the blue `Create API key` button in the top-right\n",
        "- It may generate an API directly\n",
        "  * OR: You may have the option to create a project to `Create API key in new project`. Select that one.\n",
        "  * OR: It may ask you to `Create API key in existing project`. Select one of your existing projects and select the button.\n",
        "- Copy the API key and put it into the `GEMINI_API_KEY` variable below.\n",
        "\n",
        "**You should NOT need to enter any payment methods. We will be using the FREE LLMs available**\n",
        "\n",
        "Do not share your API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "GEMINI_API_KEY = ''\n",
        "assert GEMINI_API_KEY, \"The remainder of this hands-on session needs a Gemini API key from Google's AI studio\"\n",
        "\n",
        "# Set your Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668f8461",
      "metadata": {},
      "source": [
        "## Using the API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6447a25",
      "metadata": {},
      "source": [
        "The AI studio offers a large number of models including Google's flagship Gemini LLMs. We'll use a large model that is available free: `gemma-3-27b-it`. You can read more about it [here](https://deepmind.google/models/gemma/gemma-3/). It is a lot larger than the model we ran locally earlier (27 billion parameters versus 1 billion parameters). The `it` part of its name means that it has been *instruction tuned*. That means that it can be given instructions, and not all language models can.\n",
        "\n",
        "We can use the `google.genai` interface to call the model. It loads up our API key from above and uses it. All of this is then running on remote servers, so you could use a very basic computer to do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b90ae00",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "    \n",
        "response = client.models.generate_content(\n",
        "  model=\"gemma-3-27b-it\",\n",
        "  contents=\"Tell a short bioinformatics joke\",\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d7228e",
      "metadata": {},
      "source": [
        "Great. It can tell jokes (as we well know that LLMs can do).\n",
        "\n",
        "We can get it to do the task from earlier of identifying if a sentence mentions a disease. We'll add the extra parameters including `max_output_tokens=1` to only get one token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e44b34e",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_text = \"Erlotinib is a target therapy in lung cancer.\"\n",
        "query = f\"{sentence_text}\\n\\nDoes the prior text mention a disease? Answer only Yes Or No\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemma-3-27b-it\",\n",
        "  contents=query,\n",
        "  config=genai.types.GenerateContentConfig(\n",
        "    max_output_tokens=1,\n",
        "  )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac913bec",
      "metadata": {},
      "source": [
        "Looks like it got it right. We'd expect this model to be more capable and perform better than the smaller model that we ran earlier.\n",
        "\n",
        "Prompt engineering involves adjusting the instructions given to an LLM to try to improve performance and get the outputs to align better with what we want. A powerful method for this is few-shot learning where a few examples are included in the prompt. The instructions below contain a few examples to demonstrate this idea."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cec8e187",
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_text = \"Erlotinib is a target therapy in lung cancer.\"\n",
        "\n",
        "query = f\"\"\"\n",
        "Examples:\n",
        "Aspirin treats headaches -> Yes\n",
        "The capital of France is Paris -> No\n",
        "\n",
        "{sentence_text}\n",
        "\n",
        "Does the prior text mention a disease? Answer only Yes Or No\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemma-3-27b-it\",\n",
        "  contents=query,\n",
        "  config=genai.types.GenerateContentConfig(\n",
        "    max_output_tokens=1,\n",
        "  )\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29009359",
      "metadata": {},
      "source": [
        "### üìã Task 2: Extracting chemicals\n",
        "\n",
        "The larger LLMs are much more capable of complex tasks. Let's get it to do named entity recognition for us. Create a function `extract_chemicals` that takes a string (`sentence_text`). It should run the `gemma-3-27b-it` LLM with instructions to extract a list of chemicals from the sentence text. You likely want to describe the ideal output, e.g. a comma-delimited list and nothing else. Then it will need to take the result (in `response.text`), split it appropriately (by commas if that's appropriate) and maybe remove some whitespace.\n",
        "\n",
        "Ideally we'd like the result of `extract_chemicals(\"Some chemicals include aspirin, benzene and water.\")` to be ` ['aspirin', 'benzene', 'water']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19c329fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee1f753f",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üîë Click to see the answer üîë</summary>\n",
        "\n",
        "Here is the code for the task:\n",
        "\n",
        "```python\n",
        "def extract_chemicals(sentence_text):\n",
        "    prompt = f\"\"\"\n",
        "<text>{sentence_text}</text>\n",
        "\n",
        "Extract any chemicals mentioned in the sentence above. Output the names exactly as they appear in the text. Return only the chemical names, separated by a comma and nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    client = genai.Client()\n",
        "    \n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemma-3-27b-it\",\n",
        "        contents=prompt,\n",
        "        config=genai.types.GenerateContentConfig(\n",
        "            max_output_tokens=100,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    chemicals = [ x.strip() for x in response.text.split(',') ]\n",
        "\n",
        "    return chemicals\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55d98570",
      "metadata": {},
      "source": [
        "And here's the function call that we described in the task instructions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c30af9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_chemicals(\"Some chemicals include aspirin, benzene and water.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22aae348",
      "metadata": {},
      "source": [
        "As before, we'd really like to benchmark this with an appropriate dataset. We'll turn to the [BC5CDR dataset](https://pmc.ncbi.nlm.nih.gov/articles/PMC4860626/) again and use their chemical annotations. We have pre-prepared a subset of sentences that contains the text of the sentence along with the text of the chemicals mentioned in that text. We can load it with the code below and see a few examples:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76e92a58",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data/llm_chemical_sentences.json') as f:\n",
        "  sentences = json.load(f)\n",
        "\n",
        "sentences[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bedb0d45",
      "metadata": {},
      "source": [
        "We can run our LLM on the first sentence with the code below. It gives us a good output that matches the extract sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad8a47b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_chemicals(sentences[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9047ac36",
      "metadata": {},
      "source": [
        "We want to understand how our LLM does on this task. How does it compare to an NER model trained on the BC5CDR dataset? Let's examine a model similar to what we used in the earlier hands-on session that is specialised for the BC5CDR dataset.\n",
        "\n",
        "The [Glasgow-AI4BioMed/bioner_bc5cdr](https://huggingface.co/Glasgow-AI4BioMed/bioner_bc5cdr) is a BERT-based model trained on BC5CDR. The model page provides an overview of its performance and it gets an **F1 score of 0.926** specifically for chemicals. Let's see how an LLM does when given instructions.\n",
        "\n",
        "Let's run our model on ten sentences (for time reasons). If you get a `429 RESOURCE_EXHAUSTED` error, you may want to put `time.sleep(2)` in your code as there is a request limit for this language model (30 requests per minute)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae995f99",
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "extracted = []\n",
        "for sentence in tqdm(sentences[:20]):\n",
        "    extracted.append( extract_chemicals(sentence['text']) )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ed0d6f",
      "metadata": {},
      "source": [
        "Now we need to compare the chemicals that the LLM extracted with the correct answers. The code below calculates the true positives, false positives and false negatives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7e6892",
      "metadata": {},
      "outputs": [],
      "source": [
        "TP,FP,FN = 0,0,0\n",
        "for sentence,predictions in zip(sentences,extracted):\n",
        "    TP += len(set(sentence['chemicals']).intersection(predictions)) # Number of matching chemicals in LLM predictions and correct answers\n",
        "    FN += len(set(sentence['chemicals']).difference(predictions)) # Number of missing chemicals in LLM predictions\n",
        "    FP += len(set(predictions).difference(sentence['chemicals'])) # Number of extra chemicals in LLM predictions (that aren't correct)\n",
        "    \n",
        "print(f\"{TP=} {FP=} {FN=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "641e413f",
      "metadata": {},
      "source": [
        "Those counts are a little hard to interpret. We can get some of the standard machine learning metrics (precision, recall and F1 score). Recall that the [Glasgow-AI4BioMed/bioner_bc5cdr](https://huggingface.co/Glasgow-AI4BioMed/bioner_bc5cdr) model has a F1 score of 0.926 for this task. While we're only working on a subset (so not directly comparable), let's see if we're in the same rough neighbourhood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa45acea",
      "metadata": {},
      "outputs": [],
      "source": [
        "precision = TP/(TP+FP) if (TP+FP) > 0 else 0\n",
        "recall = TP/(TP+FN) if (TP+FN) > 0 else 0\n",
        "f1 = 2*(precision*recall)/(precision+recall) if (precision+recall) > 0 else 0\n",
        "\n",
        "print(f\"{precision=:.3f} {recall=:.3f} {f1=:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ad06c32",
      "metadata": {},
      "source": [
        "Well that is much lower than the trained [BERT model](https://huggingface.co/Glasgow-AI4BioMed/bioner_bc5cdr).\n",
        "\n",
        "A few takeaways:\n",
        "\n",
        "- If you have sufficient data, training a model to do information extraction tasks (e.g. named entity recognition) will generally outperform large language models that are given instructions\n",
        "- Few-shot prompting (where you give it examples) may help you improve performance\n",
        "- But if you have no (or very little data), an LLM can be a great way to get started on extracting entity mentions - but should still be benchmarked as early as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b7c4c09",
      "metadata": {},
      "source": [
        "## LLMs and Ontologies\n",
        "\n",
        "An important task we covered earlier was entity linking. Deciding which entity is being referred to by a mention. For instance, \"GBM\" generally refers to the cancer \"glioblastoma multiforme\" in research articles. Specifically, this is entry [DOID:3068](http://www.disease-ontology.org/?id=DOID:3068) in the [Disease Ontology](https://disease-ontology.org) - the resource we used early for a list of diseases.\n",
        "\n",
        "It would be wonderful if an LLM could point us directly to the correct entry in the Disease Ontology for GBM. Let's see how `gemma-3-27b-it` does:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What Disease Ontology (DOID) term does GBM match to?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemma-3-27b-it\",\n",
        "  contents=query\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27c538d3",
      "metadata": {},
      "source": [
        "Most likely, it will not have given you DOID:3068 as the correct identifier. It will likely have hallucinated an identifier.\n",
        "\n",
        "LLMs are not good at recalling specific details (e.g. the identifier for an ontology that it may or may not have seen during training). It's better to include all the information it needs in the instructions.\n",
        "\n",
        "The example below shows another approach to entity linking where a short-list of candidates has already been identified. The LLM is asked to pick which PC4 refers to in the initial sentence. The correct answer is 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d66e0d",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "Sentence: Overexpression of PC4 enhances DNA repair efficiency by promoting the recruitment of repair factors to sites of double-strand breaks, highlighting its critical role in genome stability.\n",
        "\n",
        "1. Silicon phthalocyanine 4 (chemical) A synthetic photosensitizer agent containing a large macrocyclic ring chelated with silicon.\n",
        "2. SUB1 (gene/protein). Involved in mediating transcription induced by upstream activators. Also known as PC4.\n",
        "3. Pachyonychia Congenita 4 (disorder). A rare genetic skin disorder caused by mutations in the KRT16 gene\n",
        "4. Pericardium 4. An acupuncture point on the forearm\n",
        "\n",
        "What does PC4 refer to in the initial sentence. Answer with only the corresponding number.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemma-3-27b-it\",\n",
        "  contents=query\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71aaf3e9",
      "metadata": {},
      "source": [
        "For this sentence, it normally gets it correct. We'd need to benchmark with an appropriate dataset, but at least it didn't hallucinate ontology IDs this time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e321509",
      "metadata": {},
      "source": [
        "## The Power of LLMs\n",
        "\n",
        "The real power of LLMs is the diversity of tasks that can be thrown at them. Let's look at some other tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29746ff0",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "Summarize the abstract below into a lay summary of 2-3 sentences.\n",
        "\n",
        "Deep learning models that predict functional genomic measurements from DNA sequence are powerful tools for deciphering the genetic regulatory code. Existing methods trade off between input sequence length and prediction resolution, thereby limiting their modality scope and performance. We present AlphaGenome, which takes as input 1 megabase of DNA sequence and predicts thousands of functional genomic tracks up to single base pair resolution across diverse modalities ‚Äì including gene expression, transcription initiation, chromatin accessibility, histone modifications, transcription factor binding, chro- matin contact maps, splice site usage, and splice junction coordinates and strength. Trained on human and mouse genomes, AlphaGenome matches or exceeds the strongest respective available external models on 24 out of 26 evaluations on variant effect prediction. AlphaGenome‚Äôs ability to simultaneously score variant effects across all modalities accurately recapitulates the mechanisms of clinically-relevant variants near the TAL1 oncogene. To facilitate broader use, we provide tools for making genome track and variant effect predictions from sequence.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemma-3-27b-it\",\n",
        "  contents=query\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b910dc",
      "metadata": {},
      "source": [
        "The above code asked for a lay summary of an abstract. None of the previous methods we've explored would be able to help with this. It's a more challenging task to benchmark (i.e. what makes a good summary) but the LLM certainly seems to have had a good go at it.\n",
        "\n",
        "Another power of LLMs is the ability to output in specific formats. We will switch over to the larger `gemini-2.5-flash` model that is more powerful, but has stricter rate limits on it.\n",
        "\n",
        "Take a look at the instructions below which ask for JSON outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941b6248",
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Output five pairs of drugs and their protein targets. Output a JSON list of dictionaries where each dictionary has two keys: 'drug' and 'target'.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemini-2.5-flash\", # Switch to bigger model (with stricter rate limits)\n",
        "  contents=query,\n",
        ")\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4faf380",
      "metadata": {},
      "source": [
        "Brilliant. In this case, it followed the instructions well and the drugs and targets are output correctly in JSON that could be easily extracted. However, sometimes it may make mistakes such as outputting invalid JSON or adding extra fields. We can actually constrain the LLM to follow a specific format.\n",
        "\n",
        "The code below adds in a `response_schema` configuration that tells the output to a list of objects that match the fields found in the `DrugsAndTargets` class. This guarantees that the LLM will output data in the format desired. Notice that the format isn't even really explained in the instructions (though it may still be useful to explain any nuances)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24c4e03",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class DrugsAndTargets(BaseModel):\n",
        "  drug: str\n",
        "  target: str\n",
        "\n",
        "query = \"Output five pairs of drugs and their protein targets. Use a JSON format.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemini-2.5-flash\",\n",
        "  contents=query,\n",
        "  config={\n",
        "    \"response_mime_type\": \"application/json\",\n",
        "    \"response_schema\": list[DrugsAndTargets],\n",
        "  },\n",
        ")\n",
        "\n",
        "# Use the response as a JSON string.\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8680fe9",
      "metadata": {},
      "source": [
        "### üìã Task 3: Open information extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2941fd6",
      "metadata": {},
      "source": [
        "Another huge strength of large language models is their abilities in open information extraction where we want structured data, but we're not entirely sure of the structure or relation labels or other factors. Let's use an LLM to extract knowledge triples that contain a subject, a relation and an object. For instance, `\"paracetamol treats headache\"` could be represented as `{\"subject\":\"paracetamol\", \"relation\":\"treats\", \"object\": \"headache\"}`.\n",
        "\n",
        "The final task is to instruct `gemini-2.5-flash` to extract knowledge triples from the long paragraph below. You want the output to be a list of triples in the JSON format above (with subject, relation and object keys). Try using the `response_schema` approach to force the LLM to output in that specific format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b00b3ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "paragraph = \"\"\"\n",
        "Upon epidermal growth factor (EGF) binding, the epidermal growth factor receptor (EGFR) undergoes\n",
        "dimerization and autophosphorylation, creating docking sites for adaptor proteins like GRB2, which\n",
        "subsequently recruits SOS1 to activate RAS. Activated RAS promotes RAF1 activation, leading to a\n",
        "phosphorylation cascade involving MEK1 and ERK1/2. Phosphorylated ERK1/2 translocates to the\n",
        "nucleus, where it modulates gene expression by phosphorylating transcription factors such as ELK1.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36fa10b",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Your code goes here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838fe0b9",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>üîë Click to see the answer üîë</summary>\n",
        "\n",
        "Here is the code for the task:\n",
        "\n",
        "```python\n",
        "class Relation(BaseModel):\n",
        "  subject: str\n",
        "  relation: str\n",
        "  object: str\n",
        "\n",
        "query = f\"{paragraph}\\n\\nExtract knowledge triples from the text above in JSON format.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=\"gemini-2.5-flash\",\n",
        "  contents=query,\n",
        "  config={\n",
        "    \"response_mime_type\": \"application/json\",\n",
        "    \"response_schema\": list[Relation],\n",
        "  },\n",
        ")\n",
        "\n",
        "print(response.text)\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUBv-3aWZLFD"
      },
      "source": [
        "Fantastic. You could provide more detailed instructions about what constitutes subjects and objects, or even limit the types of relations you are interested in. But this gives an idea of how an LLM can give controlled structured outputs. Of course, you would still want some form of human evaluation to help understand whether the performance is good enough for what you want."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05258c32",
      "metadata": {},
      "source": [
        "## üìå Main Takeaways\n",
        "\n",
        "- Trained models (especially using BERT-based approaches) generally outperform large language models that have been given instructions for well-defined tasks (e.g. named entity recognition)\n",
        "- Large language models can do a vast number of tasks that would be challenging for BERT-based approaches (e.g. summarization)\n",
        "- LLMs can follow very detailed instructions, take diverse inputs and give very specific outputs\n",
        "- Human evaluation remains key to understanding the strengths and weaknesses of any approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fc5f64f",
      "metadata": {},
      "source": [
        "## üèÅ End of Hands-on Session\n",
        "\n",
        "And that brings us to the end of the session. You've learned about:\n",
        "\n",
        "- Getting a GPU from Google Colab\n",
        "- Running an LLM locally and using it to classify sentences\n",
        "- Using an LLM through an API\n",
        "- Applying an LLM for named entity recognition and its performance compared to a trained BERT model\n",
        "- The challenge of LLMs for entity linking where they may hallucinate things\n",
        "- Getting LLMs to output to very specific formats (e.g. a specific JSON structure)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c465d094",
      "metadata": {},
      "source": [
        "## üß∞ Optional Extras\n",
        "\n",
        "If you've got extra time, you could try some of the following ideas:\n",
        "\n",
        "- Try different models (e.g. the smaller gemma models) and see what effect they have. The list of models available through Google's AI Studio can be found at: https://ai.google.dev/gemini-api/docs/models\n",
        "- Learn about the text generation parameters including `top_p`, `top_k` and `temperature`. Check out: https://huggingface.co/blog/how-to-generate\n",
        "- Read about prompt engineering and try out some techniques: https://www.promptingguide.ai/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0501e4b",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
