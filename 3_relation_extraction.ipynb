{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDKMXrmUZJF1"
      },
      "source": [
        "# Relation extraction with co-occurrences and HuggingFace\n",
        "\n",
        "In this hands-on session, we'll explore identifying associations between entities in text. We'll first figure out how to find sentences that contain entities of interest, and then explore counting co-occurrences and later more complex relation extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce1555f",
      "metadata": {},
      "source": [
        "**NOTE:** If you are running this with Colab, you should make a copy for yourself. If you don't, you may lose any edits you make. To make a copy, select `File` (top-left) then `Save a Copy in Drive`. If you are not using Colab, you may need to install some prerequisites. Please see the instructions on the [Github Repo](https://github.com/Glasgow-AI4BioMed/ismb2025tutorial)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "580abbaf",
      "metadata": {},
      "source": [
        "## Getting Data\n",
        "\n",
        "As in the previous sessions, we'll download some data that we'll use later on this tutorial with the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0bea0b4",
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O data.zip https://gla-my.sharepoint.com/:u:/g/personal/jake_lever_glasgow_ac_uk/EeRHil_cAVlFo3uarbwXoi0BwlVgIO636LHF0BpIJO6Oug?download=1\n",
        "!unzip -qo data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3EVc5Tk5Sn2"
      },
      "source": [
        "## Getting documents with pre-extracted entities\n",
        "\n",
        "In the last hands-on session, we looked at applying named entity recognition methods to identify mentions of biomedical concepts (e.g. diseases, etc). [PubTator](https://www.ncbi.nlm.nih.gov/research/pubtator3/) is an existing resource which contains entity annotations (for certain types) for PubMed abstracts and PubMed Central full-text articles. Instead of running NER tools in this session, let's look at getting annotated texts from PubTator.\n",
        "\n",
        "Similar to PubMed, you can get documents through [bulk download](https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3) or through [their API](https://www.ncbi.nlm.nih.gov/research/pubtator3/api). Let's examine using their API to get a document. Below is the code to get the text and entity annotations for a single Pubmed abstract (pmid=20573926)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Example PubMed ID\n",
        "pmid = \"20573926\"\n",
        "\n",
        "# PubTator API endpoint for BioC XML\n",
        "url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pmid}\"\n",
        "\n",
        "# Make the request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check for successful response\n",
        "assert response.status_code == 200\n",
        "\n",
        "document_xml = response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe4e8c5",
      "metadata": {},
      "source": [
        "This gets the document in an XML format known as BioC XML. Let's take a quick look at what that looks like. We'll use some code to pretty print the XML which puts in nice indenting for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xml.dom.minidom import parseString\n",
        "\n",
        "dom = parseString(document_xml)\n",
        "print(dom.toprettyxml(indent=\"  \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8ca7cc",
      "metadata": {},
      "source": [
        "The XML contains documents (with the &lt;document&gt; tag). Each document could contain multiple passages (e.g. the title, the abstract and sections of the paper could be treated as separate passages). Each passage contains text, along with information about the entities annotated there (the &lt;annotation&gt; tag).\n",
        "\n",
        "Let's save it out to a file for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d892f68e",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('example.bioc.xml','w') as f:\n",
        "  f.write(document_xml)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83008b9c",
      "metadata": {},
      "source": [
        "And we'll demonstrate loading a BioC XML file using the [bioc](github.com/bionlplab/bioc) Python package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bioc import biocxml\n",
        "\n",
        "with open('example.bioc.xml') as f:\n",
        "  collection = biocxml.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "303517d1",
      "metadata": {},
      "source": [
        "Let's see how many documents are in this file. We're expecting only one as we downloaded a single one earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(collection.documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e42cfb94",
      "metadata": {},
      "source": [
        "Let's see how many passages are in this single document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "document = collection.documents[0]\n",
        "len(document.passages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aedcbd62",
      "metadata": {},
      "source": [
        "Let's see what the text is for each passage:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for passage in document.passages:\n",
        "  print(passage.text)\n",
        "  print(\"=\"*30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af23452c",
      "metadata": {},
      "source": [
        "Great. It appears to be the title and the abstract as separate passages.\n",
        "\n",
        "Let's focus on the first passage for now as we explore the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d06c1c6f",
      "metadata": {},
      "outputs": [],
      "source": [
        "passage = document.passages[0]\n",
        "passage.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1515279c",
      "metadata": {},
      "source": [
        "\n",
        "Now let's see what metadata we can get for this document. With PubTator, we can check the metadata for each passage with `.infons`. It's a dictionary containing various fields including journal, year, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "passage.infons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc9034ef",
      "metadata": {},
      "source": [
        "And how about the annotations? These are the entities that have already been identified by PubTator. These include genes, diseases, chemical, genetic variants and species.\n",
        "\n",
        "First, how many are there in this short passage?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(passage.annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a9de5da",
      "metadata": {},
      "source": [
        "Let's iterate through them and provide some details:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for anno in passage.annotations:\n",
        "  print(f\"{anno.text=}\\n{anno.infons=}\\n{anno.total_span.offset=}\\n{anno.total_span.length=}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "880ce452",
      "metadata": {},
      "source": [
        "We get similar information to the previous hands-on session with named entity recognition methods. These annotations provide the location of the entities in the text (with the `.total_span.offset` and `.total_span.length` fields). They also provide the entity type with `.infons['Gene']`. And they have done entity linking to various ontologies and resources. The identifier is accessible through `.infons['identifier']`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkMWvZKS7tBs"
      },
      "source": [
        "## Calculating co-occurrences\n",
        "\n",
        "We'll start with the most straightforward way to identify relationships between entities - that they appear in lots of documents together. Co-occurrences may not provide nuance (by telling you the specific relation) but they can still be useful to identify that two concepts (e.g. two genes) appear to be connected because they appear together. The co-occurrences could be in appearances together in sentences, paragraphs, paper abstracts or even whole papers. The granularity may depend on what you want to get, and how rare the terms are.\n",
        "\n",
        "Let's load up a set of documents that have already been annotated through the PubTator platform. We'll use them to calculate some co-occurrence counts. You can load the data with the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bioc import biocxml\n",
        "\n",
        "with open('data/cooccurrences.bioc.xml', \"r\") as f:\n",
        "    collection = biocxml.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f179ff0",
      "metadata": {},
      "source": [
        "The data is in BioC XML data format as we explored above. Let's see how many documents we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(collection.documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7469f20",
      "metadata": {},
      "source": [
        "That's a lot of documents! You need thousands (or possibly even millions) to get a good signal for identifying co-occurrences, especially for rarer terms.\n",
        "\n",
        "This dataset comes with text and also the annotations from PubTator. Let's remind ourselves what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2475df14",
      "metadata": {},
      "outputs": [],
      "source": [
        "collection.documents[0].passages[0].annotations[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f670e2f",
      "metadata": {},
      "source": [
        "They come with various fields including the text from the research paper, the database identifier that PubTator has linked them to and the entity type. In this case, `cyanogen bromide` has been linked to the MeSH database with ID: `MESH:D003488` and is a `Chemical`. You can use the MeSH Browser to view the page for [MESH:D003488](https://meshb.nlm.nih.gov/record/ui?ui=D003488).\n",
        "\n",
        "For co-occurrences, it's generally a good idea to use the normalized component (i.e. the ontology/database identifier) instead of the text form (which is 'cyanogen bromide' in this case) as the text form may vary from paper to paper. To make it easier to work with, let's make a lookup to go from the identifier to a text form.\n",
        "\n",
        "The code below creates a dictionary lookup from the type and identifier to the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quick_lookup = {}\n",
        "for i,doc in enumerate(collection.documents):\n",
        "  for passage in doc.passages:\n",
        "\n",
        "    for anno in passage.annotations:\n",
        "      quick_lookup[(anno.infons['type'],anno.infons['identifier'])] = anno.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b441bc25",
      "metadata": {},
      "source": [
        "That means we can lookup the text for the Chemical with identifier `MESH:D003488` with the code below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec466a9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "quick_lookup[('Chemical','MESH:D003488')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe7e9697",
      "metadata": {},
      "source": [
        "For a full project, it would be better to use the canonical name (i.e. the name that the entity is known by in the database) which would likely involve downloading the database (i.e. getting MeSH) or using an API to lookup the canonical name of entities.\n",
        "\n",
        "Now let's look at getting some co-occurrences. What entities are in identified in the first document in the collection? The code below outputs it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b424e9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "doc = collection.documents[0]\n",
        "\n",
        "for passage in doc.passages:\n",
        "  for anno in passage.annotations:\n",
        "    print(anno.infons['type'], anno.infons['identifier'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bff0f98c",
      "metadata": {},
      "source": [
        "While there are eight annotations, they relate to only two unique entities (a Chemical with identifier `MESH:D003488` and a Gene with identifier `213`). Let's see what those are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20da5937",
      "metadata": {},
      "outputs": [],
      "source": [
        "quick_lookup[('Chemical','MESH:D003488')], quick_lookup[('Gene','213')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f4f3ab",
      "metadata": {},
      "source": [
        "It's the `cyanogen bromide` Chemical that we had before and the `albumin` gene. In this case, we would count this as a single co-occurrence of these two entities, as they have appeared in one document. Alternatively, we could also check whether they appear in the same sentence, but we will stick to the document level at the moment.\n",
        "\n",
        "Let's look at getting co-occurrence counts at scale:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce60157",
      "metadata": {},
      "source": [
        "# Task\n",
        "\n",
        "Now it's time to go through thousands of documents in this collection and count up the number of documents that entities appear in, and the number of co-occurrences. You should get the identifiers for all the entities mentioned in each document, remove any duplicates (using a Python `set`) and then get every pair of entities. You could use the [itertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations) function to get the pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "cooccurrences = Counter()\n",
        "\n",
        "for doc in collection.documents:\n",
        "  identifiers = [ (anno.infons['type'],anno.infons['identifier']) for passage in doc.passages for anno in passage.annotations ]\n",
        "\n",
        "  unique_identifiers = sorted(set(identifiers))\n",
        "\n",
        "  for id1,id2 in itertools.combinations(unique_identifiers, 2):\n",
        "    cooccurrences[(id1,id2)] += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e0c842b",
      "metadata": {},
      "source": [
        "Let's see what the most common pairs are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9f671d",
      "metadata": {},
      "outputs": [],
      "source": [
        "cooccurrences.most_common(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46d860f2",
      "metadata": {},
      "source": [
        "Well that's difficult to understand. We can use our `quick_lookup` that we prepared earlier to get text names for each entity from their identifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for (id1,id2),count in cooccurrences.most_common(5):\n",
        "  print(f\"{id1} {quick_lookup[id1]} --- {id2} {quick_lookup[id2]} {count=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182e8a99",
      "metadata": {},
      "source": [
        "The most common co-occurrence relates to human patients and cancer, which are both frequently-occurring terms. These counts could be used for a further application to explore the connections with your genes/chemicals/etc of interest.\n",
        "\n",
        "Often it is used to log-transform co-occurrence values (as some are orders of magnitude larger than others) when dealing with them. But sometimes it's useful to use statistical tests instead."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2c5d8c3",
      "metadata": {},
      "source": [
        "# Statistical tests for Co-occurrences\n",
        "\n",
        "There is a good likelihood of two terms appearing together in the same document if one or both are very frequently occurring terms. We could use statistical tests to try to manage this and calculate a P-value for the co-occurrences being statistical chance.\n",
        "\n",
        "First, we want to know how many documents each term appears in individually. We can refashion the previous code to get those counts for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58191a62",
      "metadata": {},
      "outputs": [],
      "source": [
        "counts = Counter()\n",
        "\n",
        "for doc in collection.documents:\n",
        "  identifiers = [ (anno.infons['type'],anno.infons['identifier']) for passage in doc.passages for anno in passage.annotations ]\n",
        "\n",
        "  unique_identifiers = sorted(set(identifiers))\n",
        "\n",
        "  counts += Counter(unique_identifiers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8fc92eb",
      "metadata": {},
      "source": [
        "Let's see what the most common entities are. We're expecting human patients to be up there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65435f42",
      "metadata": {},
      "outputs": [],
      "source": [
        "counts.most_common(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58f376c5",
      "metadata": {},
      "source": [
        "Again, those are the entity identifiers from PubTator. Let's use the lookup to see what `9606` is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04a1d0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "quick_lookup[('Species', '9606')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad4ee55b",
      "metadata": {},
      "source": [
        "As expected, the most frequent term that appears is human patients that appears in 2625 documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_count = len(collection.documents)\n",
        "\n",
        "count_1_and_2 = cooccurrences[(id1, id2)]\n",
        "\n",
        "count_1_and_not_2 = counts[id1] - count_1_and_2\n",
        "\n",
        "count_2_and_not_1 = counts[id2] - count_1_and_2\n",
        "\n",
        "count_not_1_or_2 = doc_count - count_1_and_2 - count_1_and_not_2 - count_2_and_not_1\n",
        "\n",
        "# 2x2 table\n",
        "contingency_table = [[count_1_and_2, count_1_and_not_2],\n",
        "                     [count_2_and_not_1, count_not_1_or_2]]\n",
        "\n",
        "contingency_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "count_1_and_2 / (count_1_and_2+count_1_and_not_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counts[id1] / doc_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table, correction=False)\n",
        "\n",
        "p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for (id1,id2),count in cooccurrences.items():\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzFQAdvrwU8k"
      },
      "source": [
        "## Task\n",
        "\n",
        "Do that calculation at scale for a big dataset of BioC files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic10UhVC7-lz"
      },
      "source": [
        "## A rule-based approach\n",
        "\n",
        "Let's move onto figuring out the relationships between entities. It can be important to know more than if two entities appear in the same context. Is that Chemical causing or treating that Disease?\n",
        "\n",
        "Sometimes, the text may follow specific patterns (e.g. on drug labels) or we want to extract relations that are worded in highly specific ways. Let's look at using patterns for this.\n",
        "\n",
        "We'll start by looking at an example sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = {'text': 'Metformin is used for type 2 diabetes, and studies have evaluated its efficacy in polycystic ovary syndrome.',\n",
        " 'chemicals': ['Metformin'],\n",
        " 'diseases': ['type 2 diabetes', 'polycystic ovary syndrome']\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3344b524",
      "metadata": {},
      "source": [
        "The sentence follows a common pattern of `[CHEMICAL] is used for [DISEASE]`. Could we programmatically try to match that pattern for this sentence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rule = \"[CHEMICAL] is used for [DISEASE]\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32126930",
      "metadata": {},
      "source": [
        "Let's get all possible pairs of chemical and disease in this sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pairs = [ (chemical,disease) for chemical in sentence['chemicals'] for disease in sentence['diseases'] ]\n",
        "pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec715ee1",
      "metadata": {},
      "source": [
        "There are two possible pairs of chemical/disease in this sentence. Let's see if any match. We'll focus on the first one (which we do know will match):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "chemical, disease = pairs[0]\n",
        "chemical, disease"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f105dfb",
      "metadata": {},
      "source": [
        "Let's take the sentence and replace the chemical and disease with those placeholders (`[CHEMICAL]` and `[DISEASE]`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_with_placeholders = sentence['text'].replace(chemical,'[CHEMICAL]').replace(disease,'[DISEASE]')\n",
        "sentence_with_placeholders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcdcf118",
      "metadata": {},
      "source": [
        "And then check if there is a match:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rule_matches = rule in sentence_with_placeholders\n",
        "\n",
        "print(f\"Match: {rule_matches}\")\n",
        "print(f\"  [CHEMICAL]={chemical}\")\n",
        "print(f\"  [DISEASE]={disease}\")\n",
        "print(f\"  {sentence_with_placeholders}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfXa-uv8makQ"
      },
      "source": [
        "## Task\n",
        "\n",
        "The task is to come up with more rules and apply them a dataset of sentences to extract more ways of saying that a chemical treats a disease. The dataset to load is `data/rulebased_sentences.json`. When coming up with rules, you could look at sentences that don't match any rules you've already come up with.\n",
        "\n",
        "Aim to come up with rules that match with over twenty sentences!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('data/rulebased_sentences.json') as f:\n",
        "  sentences = json.load(f)\n",
        "\n",
        "rules = [\n",
        "  \"[CHEMICAL] is used to treat [DISEASE]\",\n",
        "  \"[CHEMICAL] treats [DISEASE]\",\n",
        "  \"[CHEMICAL] is effective against [DISEASE]\",\n",
        "  \"[CHEMICAL] has been shown to treat [DISEASE]\",\n",
        "  \"[CHEMICAL] therapy for [DISEASE]\",\n",
        "  \"[CHEMICAL] has therapeutic effects on [DISEASE]\",\n",
        "  \"[CHEMICAL] is indicated for the treatment of [DISEASE]\",\n",
        "  \"[CHEMICAL] is administered to manage [DISEASE]\",\n",
        "  \"[CHEMICAL] is prescribed for [DISEASE]\",\n",
        "  \"[CHEMICAL] is a treatment option for [DISEASE]\",\n",
        "  \"[CHEMICAL] can be used for [DISEASE] therapy\",\n",
        "  \"[CHEMICAL] is beneficial for patients with [DISEASE]\",\n",
        "  \"Treatment of [DISEASE] with [CHEMICAL]\",\n",
        "  \"Use of [CHEMICAL] in the treatment of [DISEASE]\",\n",
        "  \"[CHEMICAL] alleviates symptoms of [DISEASE]\"\n",
        "]\n",
        "\n",
        "counts = Counter()\n",
        "\n",
        "for sentence in sentences:\n",
        "  pairs = [ (chemical,disease) for chemical in sentence['chemicals'] for disease in sentence['diseases'] ]\n",
        "\n",
        "  for chemical,disease in pairs:\n",
        "    sentence_with_placeholders = sentence['text'].replace(chemical,'[CHEMICAL]').replace(disease,'[DISEASE]')\n",
        "\n",
        "    if any( rule in sentence_with_placeholders for rule in rules):\n",
        "      counts[(chemical,disease)] += 1\n",
        "\n",
        "len(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df86faa7",
      "metadata": {},
      "source": [
        "There are advantages and disadvantages with the rule-based approach:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "- The big advantage is that you have full control over how the relations are extracted, which may be very important for your project if specific wording is really needed. \n",
        "- It is very very fast. \n",
        "\n",
        "Disadvantages:\n",
        "- Need to write out lots of rules and the guarantee that many wordings will not match any rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HhFUjKB73UH"
      },
      "source": [
        "## A basic Open Information Extraction method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88913604",
      "metadata": {},
      "source": [
        "Sometimes you may not be sure what information you want to extract, and certainly couldn't come up with rules to extract things. Open information extraction methods are approaches that attempt to extract information without pre-determined methods. One approach to this is to extract the main verb between two entity mentions. The main verb often describes the action going on and is often the main relation that we care about.\n",
        "\n",
        "Let's set up an example sentence. We'd really like to extract the `binds` relation between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = {\n",
        "    \"text\": \"Cetuximab binds to the epidermal growth factor receptor, blocking cancer cell proliferation.\",\n",
        "    \"entities\": [\"Cetuximab\", \"epidermal growth factor receptor\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b72e5b",
      "metadata": {},
      "source": [
        "Let's figure out where the two entities of interests are in the text (i.e. their string coordinates):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pair = (\"Cetuximab\", \"epidermal growth factor receptor\")\n",
        "\n",
        "loc1 = sentence['text'].index(pair[0])\n",
        "loc2 = sentence['text'].index(pair[1])\n",
        "\n",
        "loc1, loc2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bedc0ea0",
      "metadata": {},
      "source": [
        "We can use spaCy to parse the text and gets the individual tokens with the parts-of-speech as was shown in the first hands-on session. Let's do that to this sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d851adf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc = nlp(sentence['text'])\n",
        "\n",
        "for token in doc:\n",
        "  print(f\"{token.pos_}: '{token.text}' at {token.idx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4891f90",
      "metadata": {},
      "source": [
        "We want to identify the verb that occurs between our two entities (at positions 0 and 23). Let's adjust the spaCy code to get it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for token in doc:\n",
        "    if token.pos_ == \"VERB\" and token.idx > loc1 and token.idx < loc2:\n",
        "        print(f\"{pair} {token.text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7389fd2",
      "metadata": {},
      "source": [
        "You could apply this at scale to find the main verbs between two entities occurring in the same sentence. This has some advantages and disadvantages too:\n",
        "\n",
        "Advantages:\n",
        "- You don't need to decide the relation types you want to extract up front\n",
        "- It can help explore possible information to extract\n",
        "- It can be fairly fast\n",
        "\n",
        "Disadvantages:\n",
        "- It won't catch negative cases (e.g. \"Cetuximab never binds to TERT\")\n",
        "- There is more to meaning that the main verb\n",
        "- The main verb may not be directly connecting the two entities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56353160",
      "metadata": {},
      "source": [
        "## Using a machine learning model\n",
        "\n",
        "A machine learning model can also be used to classify the relationship between two entities in some text. One challenge of this is that you need annotated data to build and evaluate your model.\n",
        "\n",
        "We will use a model that has been trained on labels generated by a larger language model. This has certain limitations as the labels will likely contain some errors. However, it demonstrates how a machine learning model can be applied. The model we will apply is [Glasgow-AI4BioMed/synthetic_relex](https://huggingface.co/Glasgow-AI4BioMed/synthetic_relex). First, let's load it up:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e60b0f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"Glasgow-AI4BioMed/synthetic_relex\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da83c14",
      "metadata": {},
      "source": [
        "The input to the model is text with the two entities of interest wrapped with [E1][/E1] and `[E2][]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cef934f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier(\"[E1]Paclitaxel[/E1] is a common chemotherapy used for [E2]lung cancer[/E2].\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCJANWC68An7"
      },
      "source": [
        "## Task\n",
        "\n",
        "Apply one of the methods above to the large set of sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data/relex_sentences.json') as f:\n",
        "  sentences = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for sentence in sentences:\n",
        "  entity1, entity2 = sentence['entities']\n",
        "\n",
        "  doc = nlp(sentence['text'])\n",
        "\n",
        "  verbs = [ (token.idx, token.text) for token in doc if token.pos_ == \"VERB\" ]\n",
        "\n",
        "  loc1 = sentence['text'].index(entity1)\n",
        "  loc2 = sentence['text'].index(entity2)\n",
        "\n",
        "  loc1,loc2 = (loc2,loc1) if loc2 < loc1 else (loc1,loc2)\n",
        "\n",
        "  verbs_between = [ verb for verb_loc,verb in verbs if verb_loc > loc1 and verb_loc < loc2 ]\n",
        "\n",
        "  if len(verbs_between) == 1:\n",
        "    print(f\"{verbs_between[0]} | {entity1} | {entity2} | {sentence['text']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0HRdo7AdQHo"
      },
      "source": [
        "## Optional Extras\n",
        "\n",
        "- Calculate p-values for each co-occurrence by creating a contigency matrix of document counts of when two entities appear (and appear together)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
